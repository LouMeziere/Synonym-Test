{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>enormously</td>\n",
       "      <td>tremendously</td>\n",
       "      <td>appropriately</td>\n",
       "      <td>uniquely</td>\n",
       "      <td>tremendously</td>\n",
       "      <td>decidedly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>provisions</td>\n",
       "      <td>stipulations</td>\n",
       "      <td>stipulations</td>\n",
       "      <td>interrelations</td>\n",
       "      <td>jurisdictions</td>\n",
       "      <td>interpretations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>haphazardly</td>\n",
       "      <td>randomly</td>\n",
       "      <td>dangerously</td>\n",
       "      <td>densely</td>\n",
       "      <td>randomly</td>\n",
       "      <td>linearly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prominent</td>\n",
       "      <td>conspicuous</td>\n",
       "      <td>battered</td>\n",
       "      <td>ancient</td>\n",
       "      <td>mysterious</td>\n",
       "      <td>conspicuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zenith</td>\n",
       "      <td>pinnacle</td>\n",
       "      <td>completion</td>\n",
       "      <td>pinnacle</td>\n",
       "      <td>outset</td>\n",
       "      <td>decline</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      question        answer              0               1              2  \\\n",
       "0   enormously  tremendously  appropriately        uniquely   tremendously   \n",
       "1   provisions  stipulations   stipulations  interrelations  jurisdictions   \n",
       "2  haphazardly      randomly    dangerously         densely       randomly   \n",
       "3    prominent   conspicuous       battered         ancient     mysterious   \n",
       "4       zenith      pinnacle     completion        pinnacle         outset   \n",
       "\n",
       "                 3  \n",
       "0        decidedly  \n",
       "1  interpretations  \n",
       "2         linearly  \n",
       "3      conspicuous  \n",
       "4          decline  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 1:\n",
    "\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "df_synonym = pd.read_csv('synonym.csv')\n",
    "df_synonym.columns\n",
    "df_synonym.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Methods for Task 1\n",
    "\n",
    "def compute_max(model, target, word1, word2, word3, word4, answer):\n",
    "    # Determines the cosine similarity between two embeddings and retrieves the closest one to the target (question-word)\n",
    "    words = [word1, word2, word3,word4]\n",
    "    max_similarity = -1  \n",
    "    selected_word = None\n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            similarity = model.similarity(word, target)\n",
    "            if similarity > max_similarity:\n",
    "                # Keep the word with the greatest similarity to the target word\n",
    "                max_similarity = similarity\n",
    "                selected_word = word\n",
    "        except KeyError:\n",
    "            print(f\"KeyError: {word}\")\n",
    "            continue\n",
    "    if selected_word is None or target not in model.index_to_key or all(word not in model.index_to_key for word in words):\n",
    "        label = 'guess'\n",
    "    elif selected_word == answer:\n",
    "        label = 'correct'\n",
    "    else:\n",
    "        label = 'wrong'\n",
    "\n",
    "    return selected_word, label\n",
    "\n",
    "\n",
    "\n",
    "def write_model_to_csv(name,model):\n",
    "    # Writes a dataframe to a file demonstrating the best predicted synonym based on a question-word.\n",
    "    # It also indicates whether the model correctly labeled the expected predicted word.\n",
    "\n",
    "    df_synonym = pd.read_csv('synonym.csv')\n",
    "\n",
    "    result_df = {\n",
    "        'question' : [],\n",
    "        'answer' : [],\n",
    "        'predictions': [],\n",
    "        'label': []\n",
    "    }\n",
    "\n",
    "    result_df['question'] = df_synonym['question']\n",
    "    result_df['answer'] = df_synonym['answer']\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    for index in range(len(df_synonym)):\n",
    "        prediction, label = compute_max(model, str(df_synonym['question'][index]), str(df_synonym['0'][index]),str(df_synonym['1'][index]), str(df_synonym['2'][index]),str(df_synonym['3'][index]),str(df_synonym['answer'][index]))\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "        labels.append(label)\n",
    "    \n",
    "    result_df['predictions'] = predictions\n",
    "    result_df['label'] = labels\n",
    "    pd.DataFrame(result_df).to_csv(f'{name}-details.csv', index=False)\n",
    "\n",
    "\n",
    "def get_analysis(name, model):\n",
    "    # Evaluates the model's performance \n",
    "    df_for_analysis = pd.read_csv(f'{name}-details.csv')\n",
    "\n",
    "    label_counts = df_for_analysis['label'].value_counts()\n",
    "    correct = label_counts.get('correct', 0)\n",
    "    wrong = label_counts.get('wrong',0)\n",
    "    guess = label_counts.get('guess', 0)\n",
    "    v = correct + wrong\n",
    "    line = f'{name}, {len(model.index_to_key)}, {correct}, {v}, {correct/v}'\n",
    "    return line\n",
    "\n",
    "def write_analysis(line):\n",
    "    # Writes the model's performance to a csv file\n",
    "    with open('analysis.csv', 'a+') as my_file:\n",
    "        my_file.write(\"Model name, vocabulary size, # of correct labels (C), # of questions answered without guessing (V), model accuracy (C/V) \\n\"\n",
    "                      + line + '\\n')\n",
    "        \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: peacefulness\n",
      "KeyError: harshness\n",
      "KeyError: weariness\n",
      "KeyError: happiness\n"
     ]
    }
   ],
   "source": [
    "# Task 1:\n",
    "\n",
    "def analysis_part_one():\n",
    "    write_model_to_csv('word2vec-google-news-300',wv)\n",
    "    line = get_analysis('word2vec-google-news-300',wv)\n",
    "    write_analysis(line)\n",
    "analysis_part_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: \n",
    "\n",
    "# Files of same size but different corpus\n",
    "crawl_subwords = api.load('fasttext-wiki-news-subwords-300')\n",
    "wiki_giga = api.load('glove-wiki-gigaword-300')\n",
    "\n",
    "# Files from the same corpus but different size\n",
    "twitter_50 = api.load('glove-twitter-50')\n",
    "twitter_100 = api.load('glove-twitter-100')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_part_two():\n",
    "    \n",
    "    write_model_to_csv('twitter-50',twitter_50)\n",
    "    string_output = get_analysis('twitter-50',twitter_50)\n",
    "    write_analysis(f'C1-C1, {string_output}')\n",
    "\n",
    "    write_model_to_csv('twitter-100',twitter_100)\n",
    "    string_output = get_analysis('twitter-100',twitter_100)\n",
    "    write_analysis(f'C2-E2, {string_output}')\n",
    "\n",
    "\n",
    "    write_model_to_csv('crawl-subwords',crawl_subwords)\n",
    "    string_output = get_analysis('crawl-subwords',crawl_subwords)\n",
    "    write_analysis(f'C3-E3, {string_output}')\n",
    "\n",
    "    write_model_to_csv('wiki-giga',wiki_giga)\n",
    "    string_output = get_analysis('wiki-giga',wiki_giga)\n",
    "    write_analysis(f'C4-E4, {string_output}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_part_two()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_three():\n",
    "\n",
    "    # Load the text from the files into a list\n",
    "    books = []\n",
    "    book_names = ['alice_in_wonderland.txt','frankenstein.txt','great_gatsby.txt','pride_and_prejudice.txt','scarlett_letter.txt']\n",
    "    for book in book_names:\n",
    "        with open(f'books/{book}', 'r', encoding='utf-8') as file:\n",
    "            try:\n",
    "                text = file.read()\n",
    "                sentences = sent_tokenize(text)\n",
    "                words = [word_tokenize(sentence) for sentence in sentences]\n",
    "                books.extend(words)\n",
    "            except:\n",
    "                continue\n",
    "    window_sizes = [50, 100]\n",
    "    embedding_sizes = [100, 300]\n",
    "    i = 0\n",
    "    for window_size in window_sizes:\n",
    "        for embedding_size in embedding_sizes:\n",
    "            model = Word2Vec(books, vector_size=embedding_size, window=window_size, min_count=1, workers=4)\n",
    "            \n",
    "            model.save(f'model{i}.model')\n",
    "            i = i+1\n",
    "    return books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "books = part_three()\n",
    "\n",
    "def analysis_part_three(books):\n",
    "    loaded_model = Word2Vec.load('model0.model')\n",
    "    loaded_model.train(books,total_examples=len(books),epochs=2)\n",
    "\n",
    "    write_model_to_csv('model0.model',loaded_model.wv)\n",
    "    string_output = get_analysis('model0.model',loaded_model.wv)\n",
    "    write_analysis(f'E5-W1 {string_output}')\n",
    "\n",
    "    loaded_model = Word2Vec.load('model1.model')\n",
    "    loaded_model.train(books,total_examples=len(books),epochs=2)\n",
    "\n",
    "    write_model_to_csv('model1.model',loaded_model.wv)\n",
    "    string_output = get_analysis('model1.model',loaded_model.wv)\n",
    "    write_analysis(f'E5-W2 {string_output}')\n",
    "\n",
    "    loaded_model = Word2Vec.load('model2.model')\n",
    "    loaded_model.train(books,total_examples=len(books),epochs=2)\n",
    "\n",
    "    write_model_to_csv('model2.model',loaded_model.wv)\n",
    "    string_output = get_analysis('model2.model',loaded_model.wv)\n",
    "    write_analysis(f'E6-W1 {string_output}')\n",
    "\n",
    "    loaded_model = Word2Vec.load('model3.model')\n",
    "    loaded_model.train(books,total_examples=len(books),epochs=2)\n",
    "\n",
    "    write_model_to_csv('model3.model',loaded_model.wv)\n",
    "    string_output = get_analysis('model3.model',loaded_model.wv)\n",
    "    write_analysis(f'E6-W2 {string_output}')\n",
    "\n",
    "    \n",
    "analysis_part_three(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model = Word2Vec.load('word2vec_model_w10_e50.model')\n",
    "#loaded_model.train(books,total_examples=len(books),epochs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
